{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArindamBanerji/wip-experiments/blob/master/nlp/ner_NER_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qqbe8odKdqk8",
        "outputId": "7f2541dd-0baa-44e5-fc8f-52f93c35858a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.9.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0Wr_x-RodqlC",
        "outputId": "ae7883b0-e95b-4e5d-c516-4ef256808276",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'4.6.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "tfds.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QpAUBmRQdqlF"
      },
      "outputs": [],
      "source": [
        "# for the time  being comment out this piece \n",
        "\n",
        "# !wget https://gmb.let.rug.nl/releases/gmb-2.2.0.zip\n",
        "\n",
        "# import wget\n",
        "url_download_from = \"https://gmb.let.rug.nl/releases/gmb-2.2.0.zip\"\n",
        "\n",
        "# wget.download('Url', 'C:\\\\PathToMyDownloadFolder\\\\NewFileName.extension')\n",
        "\n",
        "# wget.download(url_download_from, \".\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "X5QB_HxRdqlN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iuV4ymzYdqlP",
        "outputId": "ebf39138-e0d9-4f8d-d873-6965b52102bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runnig inn Google Colab : True\n"
          ]
        }
      ],
      "source": [
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "print (\"Runnig inn Google Colab :\", IN_COLAB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SSrkyCdndqlR",
        "outputId": "96dc1e18-ded5-4e74-c55d-55221645f4d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 Physical GPUs, 1 Logical GPU\n"
          ]
        }
      ],
      "source": [
        "if (IN_COLAB) :\n",
        "    ######## GPU CONFIGS FOR RTX 2070 ###############\n",
        "    ## Please ignore if not training on GPU       ##\n",
        "    ## this is important for running CuDNN on GPU ##\n",
        "\n",
        "    tf.keras.backend.clear_session() #- for easy reset of notebook state\n",
        "\n",
        "    # chck if GPU can be seen by TF\n",
        "    tf.config.list_physical_devices('GPU')\n",
        "    # only if you want to see how commands are executed, uncomment below\n",
        "    # tf.debugging.set_log_device_placement(True)\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "      # Restrict TensorFlow to only use the first GPU\n",
        "      try:\n",
        "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
        "      except RuntimeError as e:\n",
        "        # Visible devices must be set before GPUs have been initialized\n",
        "        print(e)\n",
        "    ###############################################\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "slnNGV5IfZOR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eeW3DuHhdqlU",
        "outputId": "fec18263-3290-4dca-ac00-fb7d72decaf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/NLP/NER_EXPTS\n"
          ]
        }
      ],
      "source": [
        "if (IN_COLAB) :\n",
        "    mount_base = \"/content/drive/My Drive\"\n",
        "    # Load the Drive helper and mount\n",
        "  # This will prompt for authorization.\n",
        "    drive.mount('/content/drive')\n",
        "    mount_path = \"/content/drive/My Drive/NLP/NER_EXPTS\"\n",
        "\n",
        "else :\n",
        "    mount_base = \"C:\\\\Users\\\\Arindam Banerji\\\\CopyFolder\\\\IOT_thoughts\\\\python-projects\"\n",
        "    mount_path = mount_base + \"\\\\NLP\\\\NER_EXPTS\"\n",
        "\n",
        "print (mount_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "D7vbVuT7dqlY",
        "outputId": "c74f10c0-4a39-487f-8c53-d8b61328972f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/NLP/NER_EXPTS  :  True\n"
          ]
        }
      ],
      "source": [
        "path_exists = os.path.exists(mount_path)\n",
        "\n",
        "if path_exists : \n",
        "  print (mount_path, \" : \" , path_exists )\n",
        "else:\n",
        "  print ( \"Load correct dir \", mount_path )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vb09VTctdqli",
        "outputId": "e664e8b0-d3fc-4231-c97f-7940268b46d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content\n",
            "Changed working directory: /content/drive/My Drive/NLP/NER_EXPTS\n"
          ]
        }
      ],
      "source": [
        "# Print the current working directory\n",
        "print(\"Current working directory: {0}\".format(os.getcwd()))\n",
        "\n",
        "# Change the current working directory\n",
        "os.chdir(mount_path)\n",
        "\n",
        "# Print the current working directory\n",
        "print(\"Changed working directory: {0}\".format(os.getcwd()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZgwWnkH7dqlk",
        "outputId": "3c55f774-3aa9-46b0-f107-d1730d9c25c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./gmb-2.2.0/data/\n"
          ]
        }
      ],
      "source": [
        "data_root = './gmb-2.2.0/data/'\n",
        "print ( data_root) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "D7BOVfQ6dqlm"
      },
      "outputs": [],
      "source": [
        "# data_root = './gmb-2.2.0/data/'\n",
        "\n",
        "fnames = []\n",
        "for root, dirs, files in os.walk(data_root):\n",
        "    for filename in files:\n",
        "        if filename.endswith(\".tags\"):\n",
        "            fnames.append(os.path.join(root, filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wDotrlX5dqlq",
        "outputId": "6e674bcd-02d1-41a8-d320-305e897c3cad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " num of files  10000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./gmb-2.2.0/data/p50/d0751/en.tags', './gmb-2.2.0/data/p50/d0689/en.tags']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "print (\" num of files \", len(fnames) )\n",
        "fnames[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nR909sqrdqls"
      },
      "outputs": [],
      "source": [
        "ner_path = \"./ner\"\n",
        "\n",
        "ner_exists = os.path.exists(ner_path)\n",
        "\n",
        "if ner_exists : \n",
        "    print (\"directory already exists \", ner_path)\n",
        "    shutil.rmtree(ner_path)\n",
        "\n",
        "os.mkdir(ner_path)\n",
        "\n",
        "print (\"Directory status \", ner_path, \" : \", os.path.exists(ner_path) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zAM8wMQdqlu"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import collections\n",
        " \n",
        "ner_tags = collections.Counter()\n",
        "iob_tags = collections.Counter()\n",
        "\n",
        "def strip_ner_subcat(tag):\n",
        "    # NER tags are of form {cat}-{subcat}\n",
        "    # eg tim-dow. We only want first part\n",
        "    return tag.split(\"-\")[0]\n",
        "\n",
        "\n",
        "def iob_format(ners):\n",
        "    # converts IO tags into BIO format\n",
        "    # input is a sequence of IO NER tokens\n",
        "    # convert this: O, PERSON, PERSON, O, O, LOCATION, O\n",
        "    # into: O, B-PERSON, I-PERSON, O, O, B-LOCATION, O\n",
        "    iob_tokens = []\n",
        "    for idx, token in enumerate(ners):\n",
        "        if token != 'O':  # !other\n",
        "            if idx == 0:\n",
        "                token = \"B-\" + token #start of sentence\n",
        "            elif ners[idx-1] == token:\n",
        "                token = \"I-\" + token  # continues\n",
        "            else:\n",
        "                token = \"B-\" + token\n",
        "        iob_tokens.append(token)\n",
        "        iob_tags[token] += 1\n",
        "    return iob_tokens  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGTe83Rudqlv"
      },
      "outputs": [],
      "source": [
        "total_sentences = 0\n",
        "outfiles = []\n",
        "for idx, file in enumerate(fnames):\n",
        "    with open(file, 'rb') as content:\n",
        "        data = content.read().decode('utf-8').strip()\n",
        "        sentences = data.split(\"\\n\\n\")\n",
        "        print(idx, file, len(sentences))\n",
        "        total_sentences += len(sentences)\n",
        "        \n",
        "        with open(\"./ner/\"+str(idx)+\"-\"+os.path.basename(file), \n",
        "                  'w', encoding='utf-8') as outfile:\n",
        "            outfiles.append(\"./ner/\"+str(idx)+\"-\"+os.path.basename(file))\n",
        "            writer = csv.writer(outfile)\n",
        "            \n",
        "            for sentence in sentences: \n",
        "                toks = sentence.split('\\n')\n",
        "                words, pos, ner = [], [], []\n",
        "                \n",
        "                for tok in toks:\n",
        "                    t = tok.split(\"\\t\")\n",
        "                    words.append(t[0])\n",
        "                    pos.append(t[1])\n",
        "                    ner_tags[t[3]] += 1\n",
        "                    ner.append(strip_ner_subcat(t[3]))\n",
        "                writer.writerow([\" \".join(words), \n",
        "                                 \" \".join(iob_format(ner)), \n",
        "                                 \" \".join(pos)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tJaNXc5dqly"
      },
      "outputs": [],
      "source": [
        "print(\"total number of sentences: \", total_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LADs0Is7dqlz"
      },
      "outputs": [],
      "source": [
        "print(\" nertags : \", ner_tags, \"\\n\")\n",
        "print(\"iob tags \", iob_tags, \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dk9GHVxMdql0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "labels, values = zip(*iob_tags.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgzhNH67dql1"
      },
      "outputs": [],
      "source": [
        "indexes = np.arange(len(labels))\n",
        "\n",
        "\n",
        "plt.bar(indexes, values)\n",
        "plt.xticks(indexes, labels, rotation='vertical')\n",
        "plt.margins(0.01)\n",
        "plt.subplots_adjust(bottom=0.15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0plNTatdql2"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "# could use `outfiles` param as well\n",
        "files = glob.glob(\"./ner/*.tags\")\n",
        "\n",
        "data_pd = pd.concat([pd.read_csv(f, header=None, \n",
        "                                 names=[\"text\", \"label\", \"pos\"]) \n",
        "                for f in files], ignore_index = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WohSWtwRdql2"
      },
      "outputs": [],
      "source": [
        "data_pd.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rc7hxmnOdql3"
      },
      "outputs": [],
      "source": [
        "### Keras tokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "text_tok = Tokenizer(filters='[\\\\]^\\t\\n', lower=False,\n",
        "                     split=' ', oov_token='<OOV>')\n",
        "\n",
        "pos_tok = Tokenizer(filters='\\t\\n', lower=False,\n",
        "                    split=' ', oov_token='<OOV>')\n",
        "\n",
        "ner_tok = Tokenizer(filters='\\t\\n', lower=False,\n",
        "                    split=' ', oov_token='<OOV>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzhLTkKDdql6"
      },
      "outputs": [],
      "source": [
        "text_tok.fit_on_texts(data_pd['text'])\n",
        "pos_tok.fit_on_texts(data_pd['pos'])\n",
        "ner_tok.fit_on_texts(data_pd['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PobHoRWCdql8"
      },
      "outputs": [],
      "source": [
        "ner_config = ner_tok.get_config()\n",
        "text_config = text_tok.get_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUFdK0-0dql9"
      },
      "outputs": [],
      "source": [
        "print(ner_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_BMyKikdql-"
      },
      "outputs": [],
      "source": [
        "text_vocab = eval(text_config['index_word'])\n",
        "ner_vocab = eval(ner_config['index_word'])\n",
        "\n",
        "print(\"Unique words in vocab:\", len(text_vocab))\n",
        "print(\"Unique NER tags in vocab:\", len(ner_vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2KOXxSZdql_"
      },
      "outputs": [],
      "source": [
        "x_tok = text_tok.texts_to_sequences(data_pd['text'])\n",
        "y_tok = ner_tok.texts_to_sequences(data_pd['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gS3WN_ykdql_"
      },
      "outputs": [],
      "source": [
        "print(text_tok.sequences_to_texts([x_tok[1]]), data_pd['text'][1])\n",
        "print(ner_tok.sequences_to_texts([y_tok[1]]), data_pd['label'][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vkALp1jdqmA"
      },
      "outputs": [],
      "source": [
        "print(text_tok.sequences_to_texts([x_tok[2]]), data_pd['text'][1])\n",
        "print(ner_tok.sequences_to_texts([y_tok[2]]), data_pd['label'][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHBMKDhGdqmC"
      },
      "outputs": [],
      "source": [
        "# now, pad seqences to a maximum length\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "max_len = 50\n",
        "\n",
        "x_pad = sequence.pad_sequences(x_tok, padding='post',\n",
        "                              maxlen=max_len)\n",
        "y_pad = sequence.pad_sequences(y_tok, padding='post',\n",
        "                              maxlen=max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Rtk7M-UdqmD"
      },
      "outputs": [],
      "source": [
        "print(x_pad.shape, y_pad.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_5lpSasdqmE"
      },
      "outputs": [],
      "source": [
        "text_tok.sequences_to_texts([x_pad[4]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-l1DdFEmdqmF"
      },
      "outputs": [],
      "source": [
        "ner_tok.sequences_to_texts([y_pad[4]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqUtFOJgdqmG"
      },
      "outputs": [],
      "source": [
        "num_classes = len(ner_vocab)+1\n",
        "\n",
        "Y = tf.keras.utils.to_categorical(y_pad, num_classes=num_classes)\n",
        "Y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8_KAZPVdqmG"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary \n",
        "vocab_size = len(text_vocab) + 1 \n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 64\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 100\n",
        "\n",
        "#batch size\n",
        "BATCH_SIZE=90\n",
        "\n",
        "# num of NER classes\n",
        "num_classes = len(ner_vocab)+1\n",
        "\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, TimeDistributed, Dense\n",
        "\n",
        "dropout=0.2\n",
        "def build_model_bilstm(vocab_size, embedding_dim, rnn_units, batch_size, classes):\n",
        "  model = tf.keras.Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, mask_zero=True,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    Bidirectional(LSTM(units=rnn_units,\n",
        "                           return_sequences=True,\n",
        "                           dropout=dropout,  \n",
        "                           kernel_initializer=tf.keras.initializers.he_normal())),\n",
        "    TimeDistributed(Dense(rnn_units, activation='relu')),\n",
        "    Dense(num_classes, activation=\"softmax\")\n",
        "  ])\n",
        "\n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKo1UldhdqmI"
      },
      "outputs": [],
      "source": [
        "model = build_model_bilstm(\n",
        "                        vocab_size = vocab_size,\n",
        "                        embedding_dim=embedding_dim,\n",
        "                        rnn_units=rnn_units,\n",
        "                        batch_size=BATCH_SIZE,\n",
        "                        classes=num_classes)\n",
        "model.summary()\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEkHHqkxdqmM"
      },
      "outputs": [],
      "source": [
        "X = x_pad "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjwuDHGGdqmO"
      },
      "outputs": [],
      "source": [
        "# create training and testing splits\n",
        "total_sentences = 62010\n",
        "test_size = round(total_sentences / BATCH_SIZE * 0.2)\n",
        "X_train = X[BATCH_SIZE*test_size:]\n",
        "Y_train = Y[BATCH_SIZE*test_size:]\n",
        "\n",
        "X_test = X[0:BATCH_SIZE*test_size]\n",
        "Y_test = Y[0:BATCH_SIZE*test_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DC0G7U5-dqmO"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape, Y_train.shape)\n",
        "print(X_test.shape, Y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPo0irI_dqmP"
      },
      "outputs": [],
      "source": [
        "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=15)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.8 ('kaggle_expts_venv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "be4c78cf3439baca13423b01e59fbae7d194fbbd5ee65de83dc38866596fbf17"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}