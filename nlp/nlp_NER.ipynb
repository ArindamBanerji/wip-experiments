{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOYo2kZicHuiLIuk72NhQYU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArindamBanerji/wip-experiments/blob/master/nlp/nlp_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJpI-QNAa-4-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfds.__version__"
      ],
      "metadata": {
        "id": "8o_FlV4fb8Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######## GPU CONFIGS FOR RTX 2070 ###############\n",
        "## Please ignore if not training on GPU       ##\n",
        "## this is important for running CuDNN on GPU ##\n",
        "\n",
        "tf.keras.backend.clear_session() #- for easy reset of notebook state\n",
        "\n",
        "# chck if GPU can be seen by TF\n",
        "tf.config.list_physical_devices('GPU')\n",
        "# only if you want to see how commands are executed, uncomment below\n",
        "# tf.debugging.set_log_device_placement(True)\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  # Restrict TensorFlow to only use the first GPU\n",
        "  try:\n",
        "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
        "  except RuntimeError as e:\n",
        "    # Visible devices must be set before GPUs have been initialized\n",
        "    print(e)\n",
        "###############################################"
      ],
      "metadata": {
        "id": "R4l4qWg7cOc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://gmb.let.rug.nl/releases/gmb-2.2.0.zip"
      ],
      "metadata": {
        "id": "3pu1Vy-Iclz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "RTMMJSRtiFA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mount_path = \"/content/drive/My Drive/NLP/NER\"\n",
        "\n",
        "path_exists = os.path.exists(mount_path)\n",
        "\n",
        "if path_exists : \n",
        "  print (mount_path, \" : \" , path_exists )\n",
        "else:\n",
        "  print ( \"Load correct dir \", mount_path ) "
      ],
      "metadata": {
        "id": "5jT9VeYynufd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After executing the cell above, Drive\n",
        "# files will be present in \"/content/drive/My Drive\".\n",
        "!ls \"/content/drive/My Drive/NLP/NER\""
      ],
      "metadata": {
        "id": "qZp4wuzIi1PX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "djjM0EcZjbME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_data_root = mount_dir + '/gmb-2.2.0/data/'\n",
        "print ( full_data_root) "
      ],
      "metadata": {
        "id": "QneVmw1YkEP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the current working directory\n",
        "print(\"Current working directory: {0}\".format(os.getcwd()))\n",
        "\n",
        "# Change the current working directory\n",
        "os.chdir(mount_dir)\n",
        "\n",
        "# Print the current working directory\n",
        "print(\"Current working directory: {0}\".format(os.getcwd()))"
      ],
      "metadata": {
        "id": "qgeaDvgrk5Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_root = './gmb-2.2.0/data/'\n",
        "print ( data_root) "
      ],
      "metadata": {
        "id": "iDKraQDRlSu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_root = './gmb-2.2.0/data/'\n",
        "\n",
        "fnames = []\n",
        "for root, dirs, files in os.walk(data_root):\n",
        "    for filename in files:\n",
        "        if filename.endswith(\".tags\"):\n",
        "            fnames.append(os.path.join(root, filename))"
      ],
      "metadata": {
        "id": "2gLyWgEZje36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fnames[:2]"
      ],
      "metadata": {
        "id": "YMFVcTThpj9M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}