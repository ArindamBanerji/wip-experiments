{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNcVb6KXplwgZGUUYQmY1VS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArindamBanerji/wip-experiments/blob/master/nlp/nlp_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJpI-QNAa-4-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfds.__version__"
      ],
      "metadata": {
        "id": "8o_FlV4fb8Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######## GPU CONFIGS FOR RTX 2070 ###############\n",
        "## Please ignore if not training on GPU       ##\n",
        "## this is important for running CuDNN on GPU ##\n",
        "\n",
        "tf.keras.backend.clear_session() #- for easy reset of notebook state\n",
        "\n",
        "# chck if GPU can be seen by TF\n",
        "tf.config.list_physical_devices('GPU')\n",
        "# only if you want to see how commands are executed, uncomment below\n",
        "# tf.debugging.set_log_device_placement(True)\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  # Restrict TensorFlow to only use the first GPU\n",
        "  try:\n",
        "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
        "  except RuntimeError as e:\n",
        "    # Visible devices must be set before GPUs have been initialized\n",
        "    print(e)\n",
        "###############################################"
      ],
      "metadata": {
        "id": "R4l4qWg7cOc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://gmb.let.rug.nl/releases/gmb-2.2.0.zip"
      ],
      "metadata": {
        "id": "3pu1Vy-Iclz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "RTMMJSRtiFA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os "
      ],
      "metadata": {
        "id": "adQMeIYvox58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mount_path = \"/content/drive/My Drive/NLP/NER_EXPTS\"\n",
        "\n",
        "path_exists = os.path.exists(mount_path)\n",
        "\n",
        "if path_exists : \n",
        "  print (mount_path, \" : \" , path_exists )\n",
        "else:\n",
        "  print ( \"Load correct dir \", mount_path ) "
      ],
      "metadata": {
        "id": "5jT9VeYynufd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After executing the cell above, Drive\n",
        "# files will be present in \"/content/drive/My Drive\".\n",
        "!ls \"/content/drive/My Drive/NLP/NER_EXPTS\""
      ],
      "metadata": {
        "id": "qZp4wuzIi1PX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_data_root = mount_path + '/gmb-2.2.0/data/'\n",
        "print ( full_data_root) "
      ],
      "metadata": {
        "id": "QneVmw1YkEP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the current working directory\n",
        "print(\"Current working directory: {0}\".format(os.getcwd()))\n",
        "\n",
        "# Change the current working directory\n",
        "os.chdir(mount_path)\n",
        "\n",
        "# Print the current working directory\n",
        "print(\"Current working directory: {0}\".format(os.getcwd()))"
      ],
      "metadata": {
        "id": "qgeaDvgrk5Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_root = './gmb-2.2.0/data/'\n",
        "print ( data_root) "
      ],
      "metadata": {
        "id": "iDKraQDRlSu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_root = './gmb-2.2.0/data/'\n",
        "\n",
        "fnames = []\n",
        "for root, dirs, files in os.walk(data_root):\n",
        "    for filename in files:\n",
        "        if filename.endswith(\".tags\"):\n",
        "            fnames.append(os.path.join(root, filename))"
      ],
      "metadata": {
        "id": "2gLyWgEZje36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fnames[:2]"
      ],
      "metadata": {
        "id": "YMFVcTThpj9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ner"
      ],
      "metadata": {
        "id": "P8EaKLBbqnTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import collections\n",
        " \n",
        "ner_tags = collections.Counter()\n",
        "iob_tags = collections.Counter()\n",
        "\n",
        "def strip_ner_subcat(tag):\n",
        "    # NER tags are of form {cat}-{subcat}\n",
        "    # eg tim-dow. We only want first part\n",
        "    return tag.split(\"-\")[0]\n",
        "\n",
        "\n",
        "def iob_format(ners):\n",
        "    # converts IO tags into BIO format\n",
        "    # input is a sequence of IO NER tokens\n",
        "    # convert this: O, PERSON, PERSON, O, O, LOCATION, O\n",
        "    # into: O, B-PERSON, I-PERSON, O, O, B-LOCATION, O\n",
        "    iob_tokens = []\n",
        "    for idx, token in enumerate(ners):\n",
        "        if token != 'O':  # !other\n",
        "            if idx == 0:\n",
        "                token = \"B-\" + token #start of sentence\n",
        "            elif ners[idx-1] == token:\n",
        "                token = \"I-\" + token  # continues\n",
        "            else:\n",
        "                token = \"B-\" + token\n",
        "        iob_tokens.append(token)\n",
        "        iob_tags[token] += 1\n",
        "    return iob_tokens  \n"
      ],
      "metadata": {
        "id": "CeG40yUsuks_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_sentences = 0\n",
        "outfiles = []\n",
        "for idx, file in enumerate(fnames):\n",
        "    with open(file, 'rb') as content:\n",
        "        data = content.read().decode('utf-8').strip()\n",
        "        sentences = data.split(\"\\n\\n\")\n",
        "        print(idx, file, len(sentences))\n",
        "        total_sentences += len(sentences)\n",
        "        \n",
        "        with open(\"./ner/\"+str(idx)+\"-\"+os.path.basename(file), 'w') as outfile:\n",
        "            outfiles.append(\"./ner/\"+str(idx)+\"-\"+os.path.basename(file))\n",
        "            writer = csv.writer(outfile)\n",
        "            \n",
        "            for sentence in sentences: \n",
        "                toks = sentence.split('\\n')\n",
        "                words, pos, ner = [], [], []\n",
        "                \n",
        "                for tok in toks:\n",
        "                    t = tok.split(\"\\t\")\n",
        "                    words.append(t[0])\n",
        "                    pos.append(t[1])\n",
        "                    ner_tags[t[3]] += 1\n",
        "                    ner.append(strip_ner_subcat(t[3]))\n",
        "                writer.writerow([\" \".join(words), \n",
        "                                 \" \".join(iob_format(ner)), \n",
        "                                 \" \".join(pos)])"
      ],
      "metadata": {
        "id": "hZR6JZm-uzcb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}